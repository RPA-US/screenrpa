{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/screenrpa/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ui_log_as_dataframe(log_path):\n",
    "  return pd.read_csv(log_path, sep=\";\")#, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_ui_log_as_dataframe('resources/sc_0_size50_Balanced/log_m.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase CustomClipDataset\n",
    "Definimos nuestro conjunto de datos personalizado en el que cargamos imágenes y sus etiquetas de texto asociadas desde un Dataframe.\n",
    "Invocamos el procesor de CLIP para preparar las imágenes y textos para el modelo.\n",
    "Transformarmos imágenes y textos a tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCLIPDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join('resources', 'sc_0_size50_Balanced', row['Screenshot'])\n",
    "        text_label = row['header']  \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = self.processor(text=[text_label], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        return inputs['pixel_values'].squeeze(), inputs['input_ids'].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciación del modelo clip y del procesador con los pesos preentenados del propio modelo de OpenAI. \n",
    "Estos componentes extraen las caracterísiticas visuales y lingüisticas de los datos. Con el dataset, creamos la instancia del conjunto de datos que con tiene las rutas a las imágenes y textos asociados, junto con el procesador de CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "dataset = CustomCLIPDataset(df, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de preprocesamiento de los datos. Es necesario que los textos tengan la misma longitud para su procesamiento. Se aplica padding para ello. Se facilita el entrenamiento por lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, texts = zip(*batch)\n",
    "    images_stacked = torch.stack(images)\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
    "    \n",
    "    return images_stacked, texts_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciación de DataLoader que permite automatizar la carga y preparación de datos para el entrenamiento a partir de la función que definimos anteriormente.\n",
    "Aparte, también definimos la función de perdida (CrossEntropyLoss) y un optimizador del entrenamiento con una taza de aprendizaje baja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo CLIP por épocas. Ajustar el número correspondiente en el bucle.\n",
    "Este tipo de entrenamiento a través de épocas buscas reducir la pérdida ajustando el modelo para correlacionar las imágenes con los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/screenrpa/venv/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.7255793809890747\n",
      "Epoch 1: Loss 0.97235107421875\n",
      "Epoch 2: Loss 0.5263630151748657\n",
      "Epoch 3: Loss 0.19276100397109985\n",
      "Epoch 4: Loss 0.1865340769290924\n",
      "Epoch 5: Loss 0.6931476593017578\n",
      "Epoch 6: Loss 0.6931471824645996\n",
      "Epoch 7: Loss 0.45485052466392517\n",
      "Epoch 8: Loss 0.6931471824645996\n",
      "Epoch 9: Loss 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(10): \n",
    "    for images, texts in data_loader:\n",
    "        images, texts = images.to(model.device), texts.to(model.device)\n",
    "        outputs = model(pixel_values=images, input_ids=texts)\n",
    "        loss = loss_fn(outputs.logits_per_image, outputs.logits_per_text.argmax(dim=1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss {loss.item()}\")\n",
    "model.save_pretrained(\"resources/my_finetuned_clip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
