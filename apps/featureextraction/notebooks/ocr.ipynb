{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_ocr\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"D:/01.Tesis/00. platforms/screenrpa/media/unzipped/sc_0_size100_Balanced_1714405263/sc_0_size100_Balanced\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(root_path, \"log.csv\"))\n",
    "img_names = df['Screenshot'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for C:\\Users\\Antonio\\.keras-ocr\\craft_mlt_25k.h5\n",
      "Looking for C:\\Users\\Antonio\\.keras-ocr\\crnn_kurapan.h5\n",
      "['1_img.png' '2_img.png' '3_img.png' '4_img.png' '5_img.png' '6_img.png'\n",
      " '7_img.png' '8_img.png' '9_img.png' '10_img.png' '11_img.png'\n",
      " '12_img.png' '13_img.png' '14_img.png' '15_img.png' '16_img.png'\n",
      " '17_img.png' '18_img.png' '19_img.png' '20_img.png' '21_img.png'\n",
      " '22_img.png' '23_img.png' '24_img.png' '25_img.png' '26_img.png'\n",
      " '27_img.png' '28_img.png' '29_img.png' '30_img.png' '31_img.png'\n",
      " '32_img.png' '33_img.png' '34_img.png' '35_img.png' '36_img.png'\n",
      " '37_img.png' '38_img.png' '39_img.png' '40_img.png' '41_img.png'\n",
      " '42_img.png' '43_img.png' '44_img.png' '45_img.png' '46_img.png'\n",
      " '47_img.png' '48_img.png' '49_img.png' '50_img.png' '51_img.png'\n",
      " '52_img.png' '53_img.png' '54_img.png' '55_img.png' '56_img.png'\n",
      " '57_img.png' '58_img.png' '59_img.png' '60_img.png' '61_img.png'\n",
      " '62_img.png' '63_img.png' '64_img.png' '65_img.png' '66_img.png'\n",
      " '67_img.png' '68_img.png' '69_img.png' '70_img.png' '71_img.png'\n",
      " '72_img.png' '73_img.png' '74_img.png' '75_img.png' '76_img.png'\n",
      " '77_img.png' '78_img.png' '79_img.png' '80_img.png' '81_img.png'\n",
      " '82_img.png' '83_img.png' '84_img.png' '85_img.png' '86_img.png'\n",
      " '87_img.png' '88_img.png' '89_img.png' '90_img.png' '91_img.png'\n",
      " '92_img.png' '93_img.png' '94_img.png' '95_img.png' '96_img.png'\n",
      " '97_img.png' '98_img.png' '99_img.png' '100_img.png']\n",
      "1_img.png\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "6/6 [==============================] - 8s 1s/step\n",
      "2_img.png\n",
      "1/1 [==============================] - 6s 6s/step\n"
     ]
    }
   ],
   "source": [
    "# Cargar el detector y el reconocedor pre-entrenados de keras-ocr\n",
    "pipeline = keras_ocr.pipeline.Pipeline()\n",
    "\n",
    "print(img_names)\n",
    "for img in img_names:\n",
    "    print(img)\n",
    "    if not isinstance(img, list):\n",
    "        # print(\"Solamente una imagen como entrada\")\n",
    "        images_input = [img]\n",
    "\n",
    "    # Get a set of three example images\n",
    "    images = [\n",
    "        keras_ocr.tools.read(os.path.join(root_path, path)) for path in images_input\n",
    "    ]\n",
    "    prediction_groups = pipeline.recognize(images)\n",
    "\n",
    "# Imprimir los resultados\n",
    "for image, predictions in zip(images, prediction_groups):\n",
    "    print('Resultados para una imagen:')\n",
    "    for text, box in predictions:\n",
    "        print(f'Texto: {text}, Coordenadas del cuadro: {box}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
